{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Estate: Lead Conversion\n",
    "\n",
    "## Problem Statement\n",
    "Develop a scoring system for lead conversion and derive online behavior analysis along with recommendations to client.\n",
    "\n",
    "## Data\n",
    "There are two data files - one containing the information on online beavior of the leads and the other containing \n",
    "information of those leads being converted to tenants. These two files, henceforth will be addressed as `leads_data` and \n",
    "`taregt_data` and the target column `converted_to_tenant` as `output`.\n",
    "\n",
    "## Approach\n",
    "The primary tasks to accomplish the goal of this project are:\n",
    "1. Understand the data\n",
    "2. Map `target_data` to `leads_data`\n",
    "3. Develop a scoring mechanism\n",
    "\n",
    "The solution follows a standard Data Science solution approach targeting the following sections:\n",
    "* [Exploratory Data Analysis](#eda)\n",
    "* [Data Cleaning](#data-cleaning)\n",
    "* [Feature Engineering](#feature-engineering)\n",
    "* [Model Selection and Training](#model-selection-and-training)\n",
    "* [Benchmarking Model Results](#benchmarking-model-results)\n",
    "* [Testing and Scoring](#testing-and-scoring)\n",
    "* [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# from pandas_profiling import ProfileReport\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"#eda\"></a>\n",
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_target_raw = \"./data/raw/target_data.csv\"\n",
    "target_raw = pd.read_csv(path_target_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_report = \"./reports/\"\n",
    "# if not os.path.isdir(folder_report):\n",
    "#     os.makedirs(folder_report)\n",
    "\n",
    "# path_report_target = os.path.join(folder_report, \"target.html\")\n",
    "# profile = ProfileReport(target_raw)\n",
    "# profile.to_file(path_report_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_data shape:  (266046, 2)\n",
      "number of unique values in `lead_id`: 198858\n",
      "number of unique values in `converted_to_tenant`: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"target_data shape: \", target_raw.shape)\n",
    "target_columns = target_raw.columns.tolist()\n",
    "for column in target_columns:\n",
    "    print(f\"number of unique values in `{column}`:\", target_raw[column].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_id, column_output = target_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique values in `converted_to_tenant`: [0 1]\n",
      "value count in `converted_to_tenant`:\n",
      "0    185723\n",
      "1     80323\n",
      "Name: converted_to_tenant, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"unique values in `{column_output}`:\", target_raw[column_output].unique())\n",
    "\n",
    "print(f\"value count in `{column_output}`:\")\n",
    "print(target_raw[column_output].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicate values in column `lead_id`: 67188\n",
      "duplicate rows in target_data: 51979\n"
     ]
    }
   ],
   "source": [
    "print(f\"duplicate values in column `{column_id}`:\", target_raw[column_id].duplicated().sum())\n",
    "print(f\"duplicate rows in target_data:\", target_raw.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lead ID values having more than 1 output: 15209\n"
     ]
    }
   ],
   "source": [
    "# Checking for lead ID values having more than 1 output\n",
    "target_groups = target_raw.groupby(column_id)\n",
    "target_distinct_value = target_groups[column_output].nunique()\n",
    "print(\"number of lead ID values having more than 1 output:\", len(target_distinct_value[(target_distinct_value > 1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obserations\n",
    "<ol>\n",
    "    <li> Some lead ID values have multiple outputs </li>\n",
    "    <li> There are multiple data points repeating information </li>\n",
    "</ol>\n",
    "\n",
    "### To-Do\n",
    "<ol>\n",
    "    <li> Drop lead ID values having more than one outputs </li>\n",
    "    <li> Drop duplicate rows </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leads Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Archisha\\AppData\\Local\\Temp\\ipykernel_10984\\3576564257.py:2: DtypeWarning: Columns (31,32,35,36) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  leads_raw = pd.read_csv(path_leads_raw)\n"
     ]
    }
   ],
   "source": [
    "path_leads_raw = \"./data/raw/leads_data.csv\"\n",
    "leads_raw = pd.read_csv(path_leads_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_report = \"./reports/\"\n",
    "# if not os.path.isdir(folder_report):\n",
    "#     os.makedirs(folder_report)\n",
    "\n",
    "# path_report_leads = os.path.join(folder_report, \"leads.html\")\n",
    "# profile = ProfileReport(leads_raw)\n",
    "# profile.to_file(path_report_leads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False,\n",
       " Index(['ga_lead_id'], dtype='object'),\n",
       " Index(['client_id', 'ga_lead_id'], dtype='object'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identifying a column in `leads_data` that resembles `lead_id` column in `target_data`\n",
    "column_id is leads_raw.columns, leads_raw.filter(like=column_id).columns, leads_raw.filter(like=\"id\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique IDs in `target_data`:  198858\n",
      "unique IDs in `ga_lead_id`:  3843\n",
      "common lead IDs in `target_data` and `ga_lead_id`:  3497\n",
      "unique IDs in `client_id`:  2938\n",
      "common lead IDs in `target_data` and `client_id`:  0\n"
     ]
    }
   ],
   "source": [
    "# identifying which column of `ga_lead_id` and `client_id` matches `lead_id` `target_data`\n",
    "id_target_set = set(target_raw[column_id].unique())\n",
    "print(\"unique IDs in `target_data`: \", len(id_target_set))\n",
    "\n",
    "possible__id_columns = [\"ga_lead_id\", \"client_id\"]\n",
    "for column in possible__id_columns:\n",
    "    id_leads_set = set(leads_raw[column].unique())\n",
    "    print(f\"unique IDs in `{column}`: \", len(id_leads_set))\n",
    "    id_common = id_leads_set.intersection(id_target_set)\n",
    "    print(f\"common lead IDs in `target_data` and `{column}`: \", len(id_common))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `ga_lead_id` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_id_leads = \"ga_lead_id\"\n",
    "column_id_target = column_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3553\n",
       "2     250\n",
       "3      26\n",
       "4      10\n",
       "6       2\n",
       "5       1\n",
       "Name: ga_lead_id, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking number of times the `column_id_leads` is repeated\n",
    "leads_raw[column_id_leads].value_counts().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `client_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_client = \"client_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique values in column `client_id` 2938\n",
      "null value percentage in column `client_id` 0.0\n",
      "description of number of data points for every `client_id`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    2938.000000\n",
       "mean       99.115044\n",
       "std       193.699108\n",
       "min         3.000000\n",
       "25%        30.000000\n",
       "50%        58.000000\n",
       "75%       112.000000\n",
       "max      6165.000000\n",
       "Name: client_id, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"unique values in column `{column_client}`\", leads_raw[column_client].nunique())\n",
    "print(f\"null value percentage in column `{column_client}`\", leads_raw[column_client].isna().mean())\n",
    "print(f\"description of number of data points for every `{column_client}`\")\n",
    "leads_raw[column_client].value_counts().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hypothesis\n",
    "<ol>\n",
    "    <li> A client has multiple interactions with the website </li>\n",
    "    <li> A client might be linked with multiple leads </li>\n",
    "    <li> Each `lead_id` is associated with only one client </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "leads_client_groups = leads_raw.groupby(column_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of total interactions for a sample client - 4855140000000000000: 49\n",
      "number of unique interactions for a sample client - 4855140000000000000: 34\n",
      "percentage of duplicate rows for a sample client - 4855140000000000000: 30.612244897959183\n"
     ]
    }
   ],
   "source": [
    "# hypothesis 1 testing\n",
    "random_client = random.choice(leads_raw[column_client])\n",
    "leads_client_sample = leads_client_groups.get_group(random_client)\n",
    "print(f\"number of total interactions for a sample client - {int(random_client)}:\", leads_client_sample.shape[0])\n",
    "print(f\"number of unique interactions for a sample client - {int(random_client)}:\", leads_client_sample.drop_duplicates().shape[0])\n",
    "print(f\"percentage of duplicate rows for a sample client - {int(random_client)}:\", 100 * leads_client_sample.duplicated().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of clients having more than one leads: 0.24642614023144996\n"
     ]
    }
   ],
   "source": [
    "# hypothesis 2 testing\n",
    "client_leads_count = leads_client_groups[column_id_leads].nunique()\n",
    "print(\"percentage of clients having more than one leads:\", (client_leads_count > 1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of leads with more than 1 client: 3\n",
      "percentage of leads with more than 1 client: 0.07808433107756377\n"
     ]
    }
   ],
   "source": [
    "# hypothesis 3 testing\n",
    "leads_lead_groups = leads_raw.groupby(column_id_leads)\n",
    "leads_client_count = leads_lead_groups[column_client].nunique()\n",
    "has_more_than_1_client = (leads_client_count > 1)\n",
    "print(\"number of leads with more than 1 client:\", has_more_than_1_client.sum())\n",
    "print(\"percentage of leads with more than 1 client:\", 100 * has_more_than_1_client.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hypothesis Results\n",
    "<ol>\n",
    "    <li> A client has multiple interactions with the website - positive </li>\n",
    "    <ul>\n",
    "        <li> There exists duplicate data points for a client but we do not have enough evidence to drop them </li>\n",
    "    </ul>\n",
    "    <li> A client might be linked with multiple leads - positive </li>\n",
    "    <li> Each `lead_id` is associated with only one client - negative </li>\n",
    "    <ul>\n",
    "        <li> We can drop the lead IDs having more than one clients as it constitutes 0.07% of total lead IDs </li>\n",
    "    </ul>\n",
    "</ol>\n",
    "\n",
    "<b> Note </b>\n",
    "<ul>\n",
    "    <li> A client ID is associated with more than one lead IDs, we cannot merge data on client ID to generate lead ID features </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Columns\n",
    "Analysing remainning columns of `leads_data` and grouping them based on their characteristics for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for column analysis\n",
    "def basic_info(column):\n",
    "    print(f\"--------------{column}--------------\")\n",
    "    print(f\"datatype: {leads_raw[column].dtype}\")\n",
    "    print(f\"NaN value %: {leads_raw[column].isna().mean()}\")\n",
    "    nunique = leads_raw[column].nunique()\n",
    "    if nunique <= 5:\n",
    "        print(f\"unique values: {leads_raw[column].unique()}\")\n",
    "        print(f\"value counts\")\n",
    "        print(leads_raw[column].value_counts())\n",
    "    else:\n",
    "        print(f\"number of unique values: {nunique}\")\n",
    "        if is_numeric_dtype(leads_raw[column]):\n",
    "            print(f\"minimum value: {leads_raw[column].min()}\")\n",
    "            print(f\"maximum value: {leads_raw[column].max()}\")\n",
    "    print(f\"sample client data: {leads_client_groups.get_group(random.choice(leads_raw[column_client]))[column].tolist()[:10]}\")\n",
    "    print(f\"--------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------sessions--------------\n",
      "datatype: int64\n",
      "NaN value %: 0.0\n",
      "unique values: [0 1 2 3 4]\n",
      "value counts\n",
      "0    273049\n",
      "1     18060\n",
      "2        78\n",
      "3         9\n",
      "4         4\n",
      "Name: sessions, dtype: int64\n",
      "sample client data: [0, 0, 0, 1, 0, 0, 0, 0, 0, 1]\n",
      "--------------------------------------\n",
      "--------------timestamp--------------\n",
      "datatype: object\n",
      "NaN value %: 0.7170089285714286\n",
      "number of unique values: 72\n",
      "sample client data: ['15/09/2021 8:41', '15/09/2021 8:41', nan, '15/09/2021 8:41', '15/09/2021 8:41', nan, nan, nan, '15/09/2021 8:41', nan]\n",
      "--------------------------------------\n",
      "--------------hits--------------\n",
      "datatype: float64\n",
      "NaN value %: 0.7170089285714286\n",
      "number of unique values: 55\n",
      "minimum value: 1.0\n",
      "maximum value: 358.0\n",
      "sample client data: [7.0, nan, nan, nan, nan, nan, 15.0, nan, 4.0, 4.0]\n",
      "--------------------------------------\n",
      "--------------unique_events--------------\n",
      "datatype: int64\n",
      "NaN value %: 0.0\n",
      "number of unique values: 24\n",
      "minimum value: 0\n",
      "maximum value: 74\n",
      "sample client data: [1, 3, 3, 10, 1, 4, 2, 2, 0, 1]\n",
      "--------------------------------------\n",
      "--------------session_duration--------------\n",
      "datatype: float64\n",
      "NaN value %: 0.7170089285714286\n",
      "number of unique values: 2208\n",
      "minimum value: 0.0\n",
      "maximum value: 32289.0\n",
      "sample client data: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "--------------------------------------\n",
      "--------------page_per_session--------------\n",
      "datatype: float64\n",
      "NaN value %: 0.7170089285714286\n",
      "unique values: [ 0. nan  1.]\n",
      "value counts\n",
      "0.0    74432\n",
      "1.0     7975\n",
      "Name: page_per_session, dtype: int64\n",
      "sample client data: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "--------------------------------------\n",
      "--------------enquiries--------------\n",
      "datatype: float64\n",
      "NaN value %: 0.7170089285714286\n",
      "unique values: [ 0. nan  1.]\n",
      "value counts\n",
      "0.0    81523\n",
      "1.0      884\n",
      "Name: enquiries, dtype: int64\n",
      "sample client data: [nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.0]\n",
      "--------------------------------------\n",
      "--------------social_source_referral--------------\n",
      "datatype: object\n",
      "NaN value %: 0.8632554945054945\n",
      "unique values: ['No' nan 'Yes']\n",
      "value counts\n",
      "No     38526\n",
      "Yes     1294\n",
      "Name: social_source_referral, dtype: int64\n",
      "sample client data: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "--------------------------------------\n",
      "--------------pageviews--------------\n",
      "datatype: float64\n",
      "NaN value %: 0.8632554945054945\n",
      "number of unique values: 11\n",
      "minimum value: 0.0\n",
      "maximum value: 12.0\n",
      "sample client data: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "--------------------------------------\n",
      "--------------time_on_page--------------\n",
      "datatype: float64\n",
      "NaN value %: 0.8632554945054945\n",
      "number of unique values: 844\n",
      "minimum value: 0.0\n",
      "maximum value: 2897.0\n",
      "sample client data: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "--------------------------------------\n",
      "--------------avg_time_on_page--------------\n",
      "datatype: float64\n",
      "NaN value %: 0.8632554945054945\n",
      "number of unique values: 1274\n",
      "minimum value: 0.0\n",
      "maximum value: 1881.0\n",
      "sample client data: [nan, 87.0, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "--------------------------------------\n",
      "--------------contact_us_click--------------\n",
      "datatype: float64\n",
      "NaN value %: 0.8632554945054945\n",
      "unique values: [ 0. nan  1.]\n",
      "value counts\n",
      "0.0    39800\n",
      "1.0       20\n",
      "Name: contact_us_click, dtype: int64\n",
      "sample client data: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "--------------------------------------\n",
      "--------------contact_us_form--------------\n",
      "datatype: float64\n",
      "NaN value %: 0.8632554945054945\n",
      "unique values: [ 0. nan]\n",
      "value counts\n",
      "0.0    39820\n",
      "Name: contact_us_form, dtype: int64\n",
      "sample client data: [0.0, 0.0, nan, nan, 0.0, nan, nan, nan, nan, nan]\n",
      "--------------------------------------\n",
      "--------------app_downloads--------------\n",
      "datatype: float64\n",
      "NaN value %: 0.8632554945054945\n",
      "unique values: [ 0. nan  1.]\n",
      "value counts\n",
      "0.0    39763\n",
      "1.0       57\n",
      "Name: app_downloads, dtype: int64\n",
      "sample client data: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "--------------------------------------\n",
      "--------------pdf_download--------------\n",
      "datatype: float64\n",
      "NaN value %: 0.8708207417582418\n",
      "unique values: [ 0. nan]\n",
      "value counts\n",
      "0.0    37617\n",
      "Name: pdf_download, dtype: int64\n",
      "sample client data: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "--------------------------------------\n",
      "--------------website_search--------------\n",
      "datatype: float64\n",
      "NaN value %: 0.8708207417582418\n",
      "unique values: [ 0. nan]\n",
      "value counts\n",
      "0.0    37617\n",
      "Name: website_search, dtype: int64\n",
      "sample client data: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "--------------------------------------\n",
      "--------------social_media_click--------------\n",
      "datatype: float64\n",
      "NaN value %: 0.8708207417582418\n",
      "unique values: [ 0. nan]\n",
      "value counts\n",
      "0.0    37617\n",
      "Name: social_media_click, dtype: int64\n",
      "sample client data: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "--------------------------------------\n",
      "--------------property_search--------------\n",
      "datatype: float64\n",
      "NaN value %: 0.8708207417582418\n",
      "unique values: [ 0. nan  1.]\n",
      "value counts\n",
      "0.0    37140\n",
      "1.0      477\n",
      "Name: property_search, dtype: int64\n",
      "sample client data: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "--------------------------------------\n",
      "--------------lead_dropped--------------\n",
      "datatype: int64\n",
      "NaN value %: 0.0\n",
      "unique values: [0 1]\n",
      "value counts\n",
      "0    286727\n",
      "1      4473\n",
      "Name: lead_dropped, dtype: int64\n",
      "sample client data: [0, 0, 0, 0, 0, 1, 0, 0, 1, 0]\n",
      "--------------------------------------\n",
      "--------------date_hour_minute--------------\n",
      "datatype: float64\n",
      "NaN value %: 0.0\n",
      "unique values: [2.02109e+11 2.02108e+11]\n",
      "value counts\n",
      "2.021090e+11    179072\n",
      "2.021080e+11    112128\n",
      "Name: date_hour_minute, dtype: int64\n",
      "sample client data: [202108000000.0, 202108000000.0, 202108000000.0, 202108000000.0, 202108000000.0, 202108000000.0, 202108000000.0, 202108000000.0, 202108000000.0, 202108000000.0]\n",
      "--------------------------------------\n",
      "--------------source--------------\n",
      "datatype: object\n",
      "NaN value %: 0.7170089285714286\n",
      "number of unique values: 53\n",
      "sample client data: [nan, 'google', 'google', 'google', 'google', nan, nan, nan, nan, nan]\n",
      "--------------------------------------\n",
      "--------------medium--------------\n",
      "datatype: object\n",
      "NaN value %: 0.7170089285714286\n",
      "number of unique values: 24\n",
      "sample client data: [nan, 'organic', 'organic', 'organic', nan, nan, nan, 'organic', 'organic', nan]\n",
      "--------------------------------------\n",
      "--------------campaign--------------\n",
      "datatype: object\n",
      "NaN value %: 0.7170089285714286\n",
      "number of unique values: 40\n",
      "sample client data: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "--------------------------------------\n",
      "--------------bounce_rate--------------\n",
      "datatype: float64\n",
      "NaN value %: 0.7170089285714286\n",
      "unique values: [  0.  nan 100.  50.]\n",
      "value counts\n",
      "0.0      81364\n",
      "100.0     1042\n",
      "50.0         1\n",
      "Name: bounce_rate, dtype: int64\n",
      "sample client data: [nan, nan, 0.0, nan, nan, 0.0, 0.0, 0.0, nan, nan]\n",
      "--------------------------------------\n",
      "--------------social_source_referral.1--------------\n",
      "datatype: object\n",
      "NaN value %: 0.8632554945054945\n",
      "unique values: ['No' nan 'Yes']\n",
      "value counts\n",
      "No     38526\n",
      "Yes     1294\n",
      "Name: social_source_referral.1, dtype: int64\n",
      "sample client data: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "--------------------------------------\n",
      "--------------device_category--------------\n",
      "datatype: object\n",
      "NaN value %: 0.8632657967032967\n",
      "unique values: ['mobile' nan 'tablet']\n",
      "value counts\n",
      "mobile    39181\n",
      "tablet      636\n",
      "Name: device_category, dtype: int64\n",
      "sample client data: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "--------------------------------------\n",
      "--------------data_source--------------\n",
      "datatype: object\n",
      "NaN value %: 0.8632657967032967\n",
      "unique values: ['web' nan]\n",
      "value counts\n",
      "web    39817\n",
      "Name: data_source, dtype: int64\n",
      "sample client data: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "--------------------------------------\n",
      "--------------event_category--------------\n",
      "datatype: object\n",
      "NaN value %: 0.21358173076923076\n",
      "number of unique values: 20\n",
      "sample client data: ['Property View', 'Client ID', 'Client ID', 'Property Search', 'Property View', 'Client ID', 'Client ID', 'Property Search', nan, nan]\n",
      "--------------------------------------\n",
      "--------------event_label--------------\n",
      "datatype: object\n",
      "NaN value %: 0.21358173076923076\n",
      "number of unique values: 10936\n",
      "sample client data: ['RES', '25%', '50%', '75%', '25%', 'RES', 'RES', nan, nan, nan]\n",
      "--------------------------------------\n",
      "--------------event_action--------------\n",
      "datatype: object\n",
      "NaN value %: 0.21358173076923076\n",
      "number of unique values: 7187\n",
      "sample client data: [nan, nan, nan, nan, '1150960275', 'undefined', '1150960275', 'View', '1150960275', 'View']\n",
      "--------------------------------------\n",
      "--------------property_view_location--------------\n",
      "datatype: object\n",
      "NaN value %: 0.9999175824175824\n",
      "number of unique values: 8\n",
      "sample client data: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "--------------------------------------\n",
      "--------------property_view_price--------------\n",
      "datatype: object\n",
      "NaN value %: 0.9999175824175824\n",
      "number of unique values: 13\n",
      "sample client data: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "--------------------------------------\n",
      "--------------total_events--------------\n",
      "datatype: float64\n",
      "NaN value %: 0.8632554945054945\n",
      "number of unique values: 36\n",
      "minimum value: 0.0\n",
      "maximum value: 37.0\n",
      "sample client data: [4.0, 4.0, 4.0, 4.0, 4.0, nan, nan, nan, nan, nan]\n",
      "--------------------------------------\n",
      "--------------property_view_size--------------\n",
      "datatype: object\n",
      "NaN value %: 0.9999175824175824\n",
      "number of unique values: 13\n",
      "sample client data: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "--------------------------------------\n",
      "--------------property_view_bedrooms--------------\n",
      "datatype: object\n",
      "NaN value %: 0.9999175824175824\n",
      "number of unique values: 7\n",
      "sample client data: [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "other_columns = [column for column in leads_raw.columns if column not in [column_id_leads, column_client]]\n",
    "for column in other_columns:\n",
    "    basic_info(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping columns based on above observation\n",
    "leads_features_info = {\n",
    "    \"1\" : {\"columns\": [\"sessions\", \"hits\", \"unique_events\",\n",
    "                       \"session_duration\", \"page_per_session\", \"enquiries\",\n",
    "                       \"pageviews\", \"time_on_page\", \"avg_time_on_page\",\n",
    "                       \"property_search\", \"lead_dropped\", \"bounce_rate\",\n",
    "                       \"total_events\"\n",
    "                       ],},\n",
    "    \"2\" : {\"columns\": [\"timestamp\", ],},\n",
    "    \"3\" : {\"columns\": [\"social_source_referral\" ],},\n",
    "    \"4\" : {\"columns\": [\"contact_us_click\", \"app_downloads\"],},\n",
    "    \"5\" : {\"columns\": [\"source\", ], \"source_mapping\": {'google': \"google\", 'Google': \"search\", '(direct)': \"direct\", 'eservices.client.ae': \"client\", 'client.ae': \"client\", 'Property Finder': \"leasing_website\", 'instagram.com': \"chat_media\", 'PropertyFinder': \"leasing_website\", 'facebook': \"chat_media\", 'client.com': \"client\", 'ecservices.client.ae': \"client\", 'SMS': \"chat_media\", 'PF_Desktop': \"leasing_website\", 'l.facebook.com': \"chat_media\", 'Bayut': \"leasing_website\", 'Facebook': \"chat_media\", 'tpc.googlesyndication.com': \"google\", 'm.facebook.com': \"chat_media\", 'bing': \"search\", 'burpsuite': \"leasing_website\", 'Email': \"email\", 'Ffvtabghef': \"email\", 'clientretail.com': \"client\", 'Esignature': \"email\", 'acm2.eim.ae': \"email\", 'eservicesqa.client.ae': \"client\", 'teams.microsoft.com': \"chat_media\", 'c3.avaamo.com': \"email\", 'Outsource': \"leasing_website\", 'testsecureacceptance.cybersource.com': \"leasing_website\", 'mail.google.com': \"email\", 'facebook.com': \"chat_media\", 'email signature': \"email\", 'dreclands.ae': \"leasing_website\", 'gulfnews': \"search\", 'clientdistrict.client.ae': \"client\", 'yahoo': \"search\", 'tagassistant.google.com': \"google\", 'm.nearbyme.io': \"leasing_website\", 'client51.client.ae': \"client\", 'developers.google.com': \"google\", 'duckduckgo': \"search\", 'clientqa.client.com': \"client\", 'login.microsoftonline.com': \"email\", 'search-dra.dt.dbankcloud.com': \"email\", 'analytics.google.com': \"google\", 'qwant.com': \"search\", 'Email-clientdistrict-buildingD': \"email\", 'korter.ae': \"leasing_website\",'linkedin.com': \"chat_media\", 'Bayut_Desktop': \"leasing_website\", 'lm.facebook.com': \"chat_media\", 'l.instagram.com': \"chat_media\",}},\n",
    "    \"6\" : {\"columns\": [\"device_category\"], \"devices\": [device for device in leads_raw[\"device_category\"].unique().tolist() if device is not np.NaN]},\n",
    "    \"7\" : {\"columns\": [\"event_category\"], \"categories\": [cat for cat in leads_raw[\"event_category\"].unique().tolist() if cat is not np.NaN]},\n",
    "}\n",
    "# columns to drop which are not providing any relevent information\n",
    "leads_columns_to_drop = [\"contact_us_form\", \"pdf_download\", \"website_search\", \n",
    "                   \"social_media_click\", \"date_hour_minute\", \"medium\",\n",
    "                   \"data_source\", \"property_view_location\",\n",
    "                   \"property_view_price\", \"property_view_size\",\n",
    "                   \"property_view_bedrooms\"]\n",
    "# columns need more analysis than provided by `basic_info` \n",
    "leads_columns_need_more_analysis = [\"campaign\", \"event_label\", \"event_action\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obserations\n",
    "<ol>\n",
    "    <li> `lead_id` of `target_data` does not have the same column name in `leads_data` </li>\n",
    "    <li> There are no common IDs in `target_data` and `client_id`, hence we will use `ga_lead_id` to merge with the `target_data` </li>\n",
    "    <li> Number of leads are more than number of clients, hence a client might be associated with more than one lead ID </li>\n",
    "    <li> We have grouped columns based on their characteristics and identified which to drop and those which need advanced analysis </li>\n",
    "</ol>\n",
    "\n",
    "### To-Do\n",
    "<ol>\n",
    "    <li> Drop lead IDs from `target_data` having more than one client ID </li>\n",
    "    <li> Features to be created </li>\n",
    "    <ul>\n",
    "        <li> Number of leads for a client </li>\n",
    "        <li> Number of interactions for a client </li>\n",
    "        <li> Normalize required numerical features by number of leads for a client </li>\n",
    "        <li> Generate features based on column grouping </li>\n",
    "    </ul>\n",
    "    <li> Drop columns identified in column analysis </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the `target_data` after dropping lead ID values having more than one outputs: (226847, 2)\n"
     ]
    }
   ],
   "source": [
    "# dropping lead ID values having more than one outputs\n",
    "# target_groups_filtered = target_groups.filter(lambda x: x[col_label].nunique()==1)\n",
    "lead_ids_to_drop = target_distinct_value[~(target_distinct_value==1)].index.tolist()\n",
    "target_filtered = target_raw[~target_raw[column_id_target].isin(lead_ids_to_drop)]\n",
    "print(\"shape of the `target_data` after dropping lead ID values having more than one outputs:\", target_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the `target_data` after dropping duplicate rows: (183649, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Archisha\\AppData\\Local\\Temp\\ipykernel_10984\\313814757.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  target_filtered.drop_duplicates(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# dropping duplicate rows\n",
    "target_filtered.drop_duplicates(inplace=True)\n",
    "print(\"shape of the `target_data` after dropping duplicate rows:\", target_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the `target_data` after dropping lead ID values having more than one clients in `leads_data`: (183646, 2)\n"
     ]
    }
   ],
   "source": [
    "# dropping lead ID values having more than one clients in Leads Data\n",
    "leads_groups = leads_raw.groupby(column_id_leads)\n",
    "leads_clients = leads_groups[column_client].nunique()\n",
    "leads_to_drop = leads_clients[(leads_clients>1)].index.tolist()\n",
    "target_filtered = target_filtered[~(target_filtered[column_id_target].isin(leads_to_drop))]\n",
    "print(\"shape of the `target_data` after dropping lead ID values having more than one clients in `leads_data`:\", target_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving filtered Target Data\n",
    "path_target_filtered = \"./data/target.csv\"\n",
    "target_filtered.to_csv(path_target_filtered, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "del target_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leads Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns \n",
    "leads_filtered = leads_raw.drop(columns=leads_columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "leads_filtered[\"timestamp\"] = pd.to_datetime(leads_filtered[\"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_leads_filtered = \"./data/leads.csv\"\n",
    "leads_filtered.to_csv(path_leads_filtered, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "del leads_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing Features\n",
    "Based on the column segregation in [Other Column](#column-analysis) we are designing features for different sets of groups.\n",
    "\n",
    "<b>`feature_set1`</b> <br>\n",
    "<i>Numeric features</i>\n",
    "<ol>\n",
    "    <li> sum </li>\n",
    "    <li> mean </li>\n",
    "    <li> not na rows </li>\n",
    "    <li> non-zero rows </li>\n",
    "    <li> max </li>\n",
    "    <li> sum / leads for a client </li>\n",
    "    <li> not na rows / leads for a client </li>\n",
    "    <li> non-zero rows / leads for a client </li>\n",
    "</ol>\n",
    "\n",
    "<b>`feature_set2`</b><br>\n",
    "<i>Datetime features</i>\n",
    "<ol>\n",
    "    <li> Timegap feature based on `timestamp` - gaps in minutes between first and last interaction </li>\n",
    "</ol>\n",
    "\n",
    "<b>`feature_set3`</b><br>\n",
    "<i>Boolean feature</i>\n",
    "<ol>\n",
    "    <li> If client has `social_source_referral` </li>\n",
    "</ol>\n",
    "\n",
    "<b>`feature_set4`</b><br>\n",
    "<i>Boolean features</i>\n",
    "<ol>\n",
    "    <li> If client has clicked on contact-us link (`contact_us_click`) and downloaded app (`app_downloads`)</li>\n",
    "</ol>\n",
    "\n",
    "<b>`feature_set5`</b><br>\n",
    "<i>Categorical and Numeric features</i>\n",
    "<ol>\n",
    "    <li> Using the extended list of `sources` to map down to smaller feature set </li>\n",
    "    <li> Finding most used `source` for a user </li>\n",
    "    <li> Counting occurence of every `source` </li>\n",
    "</ol>\n",
    "\n",
    "<b>`feature_set6`</b><br>\n",
    "<i>Numeric features</i>\n",
    "<ol>\n",
    "    <li> Count of devices (mobile, tablet) used for accessing the website </li>\n",
    "</ol>\n",
    "\n",
    "<b>`feature_set7`</b><br>\n",
    "<i>Numeric features</i>\n",
    "<ol>\n",
    "    <li> Count of `event_category` </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_not_na_count(series):\n",
    "    return (~series.isna()).sum()\n",
    "\n",
    "def get_nonzero(series):\n",
    "    return ((series!=0) & (~series.isna())).sum()\n",
    "\n",
    "def get_feature_set1(group, column, column_merge):\n",
    "    # sum of column\n",
    "    features = group[column].sum().reset_index()\n",
    "    features.columns = [column_merge, f\"{column}-sum-{column_merge}\"]\n",
    "    # number of not null rows\n",
    "    feature2 = group[column].apply(get_not_na_count).reset_index()\n",
    "    feature2.columns = [column_merge, f\"{column}-count_not_na-{column_merge}\"]\n",
    "    features = features.merge(feature2, on=column_merge, how=\"outer\")\n",
    "    # max of column\n",
    "    feature3 = group[column].max().reset_index()\n",
    "    feature3.columns = [column_merge, f\"{column}-max-{column_merge}\"]\n",
    "    features = features.merge(feature3, on=column_merge, how=\"outer\")\n",
    "    # counting non zero rows\n",
    "    feature4 = group[column].apply(get_nonzero).reset_index()\n",
    "    feature4.columns = [column_merge, f\"{column}-count_nonzero-{column_merge}\"]\n",
    "    features = features.merge(feature4, on=column_merge, how=\"outer\")\n",
    "    # mean of column\n",
    "    features[f\"{column}-mean-{column_merge}\"] = features[f\"{column}-sum-{column_merge}\"] / features[f\"{column}-count_not_na-{column_merge}\"]\n",
    "\n",
    "    group_id_leads = group[column_id_leads].nunique().reset_index(drop=True)\n",
    "    # sum/ #leads\n",
    "    features[f\"{column}-sum_per_lead-{column_merge}\"] = features[f\"{column}-sum-{column_merge}\"] / group_id_leads\n",
    "    # #not_na_rows/ #leads\n",
    "    features[f\"{column}-count_not_na_per_lead-{column_merge}\"] = features[f\"{column}-count_not_na-{column_merge}\"] / group_id_leads\n",
    "    # #not_zero_rows/ #leads\n",
    "    features[f\"{column}-count_nonzero_per_lead-{column_merge}\"] = features[f\"{column}-count_nonzero-{column_merge}\"] / group_id_leads\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_set2(group, column, column_merge):\n",
    "    # calculating time gap in minutes between first and last interaction\n",
    "    features = ((group[column].max() - group[column].min()).dt.total_seconds() / 60).reset_index()\n",
    "    features.columns = [column_merge, f\"{column}-gap-{column_merge}\"]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_in_series(value, series):\n",
    "    return value in series.tolist()\n",
    "\n",
    "def get_feature_set3(group, column, column_merge):\n",
    "    # checking if \"Yes\" exists in a column\n",
    "    features = group[column].apply(partial(value_in_series, \"Yes\")).reset_index()\n",
    "    features.columns = [column_merge, f\"{column}-value-{column_merge}\"]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_set4(group, column, column_merge):\n",
    "    # checking if \"1\" exists in a column\n",
    "    features = group[column].apply(partial(value_in_series, 1)).reset_index()\n",
    "    features.columns = [column_merge, f\"{column}-value-{column_merge}\"]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "set5_source_mapping = leads_features_info[\"5\"][\"source_mapping\"]\n",
    "def get_series_mode(series):\n",
    "    mode = series.map(set5_source_mapping).mode()\n",
    "    if len(mode):\n",
    "        return mode.iloc[0]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def get_series_count(value, series):\n",
    "    return (series == value).sum()\n",
    "\n",
    "def get_feature_set5(group, column, column_merge):\n",
    "    # finding most used source\n",
    "    features = group[column].apply(get_series_mode).reset_index()\n",
    "    features.columns = [column_merge, f\"{column}-mode-{column_merge}\"]\n",
    "    # creating a feature for each source count\n",
    "    for value in set(set5_source_mapping.values()):\n",
    "        feature = group[column].apply(partial(get_series_count, value)).reset_index()\n",
    "        feature.columns = [column_merge, f\"{column}-{value}-count-{column_merge}\"]\n",
    "        features = features.merge(feature, on=column_merge, how=\"outer\")\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_value(value, series):\n",
    "    return (series == value).sum()\n",
    "\n",
    "def get_feature_set6(group, column, column_merge):\n",
    "    # count of devices\n",
    "    values = leads_features_info[\"6\"][\"devices\"]\n",
    "    features = pd.DataFrame({column_merge: []})\n",
    "    for value in values:\n",
    "        feature = group[column].apply(partial(is_value, value)).reset_index()\n",
    "        feature.columns = [column_merge, f\"{column}-count_{value}-{column_merge}\"]\n",
    "        features = features.merge(feature, on=column_merge, how=\"outer\")\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_set7(group, column, column_merge):\n",
    "    # count of categories\n",
    "    values = leads_features_info[\"7\"][\"categories\"]\n",
    "    features = pd.DataFrame({column_merge: []})\n",
    "    for value in values:\n",
    "        feature = group[column].apply(partial(is_value, value)).reset_index()\n",
    "        feature.columns = [column_merge, f\"{column}-count_{value}-{column_merge}\"]\n",
    "        features = features.merge(feature, on=column_merge, how=\"outer\")\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "leads_features_info[\"1\"][\"feature_function\"] = get_feature_set1\n",
    "leads_features_info[\"2\"][\"feature_function\"] = get_feature_set2\n",
    "leads_features_info[\"3\"][\"feature_function\"] = get_feature_set3\n",
    "leads_features_info[\"4\"][\"feature_function\"] = get_feature_set4\n",
    "leads_features_info[\"5\"][\"feature_function\"] = get_feature_set5\n",
    "leads_features_info[\"6\"][\"feature_function\"] = get_feature_set6\n",
    "leads_features_info[\"7\"][\"feature_function\"] = get_feature_set7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating features for set: 1 column: sessions\n",
      "generating features for set: 1 column: hits\n",
      "generating features for set: 1 column: unique_events\n",
      "generating features for set: 1 column: session_duration\n",
      "generating features for set: 1 column: page_per_session\n",
      "generating features for set: 1 column: enquiries\n",
      "generating features for set: 1 column: pageviews\n",
      "generating features for set: 1 column: time_on_page\n",
      "generating features for set: 1 column: avg_time_on_page\n",
      "generating features for set: 1 column: property_search\n",
      "generating features for set: 1 column: lead_dropped\n",
      "generating features for set: 1 column: bounce_rate\n",
      "generating features for set: 1 column: total_events\n",
      "generating features for set: 2 column: timestamp\n",
      "generating features for set: 3 column: social_source_referral\n",
      "generating features for set: 4 column: contact_us_click\n",
      "generating features for set: 4 column: app_downloads\n",
      "generating features for set: 5 column: source\n",
      "generating features for set: 6 column: device_category\n",
      "generating features for set: 7 column: event_category\n"
     ]
    }
   ],
   "source": [
    "feature_client = pd.DataFrame({column_client: []})\n",
    "leads_client = leads_filtered.groupby(column_client)\n",
    "\n",
    "for feature_set in leads_features_info:\n",
    "    feature_set_function = leads_features_info[feature_set][\"feature_function\"]\n",
    "    for column in leads_features_info[feature_set][\"columns\"]:\n",
    "        print(\"generating features for set:\", feature_set, \"column:\", column)\n",
    "        features_set_column = feature_set_function(leads_client, column, column_client)\n",
    "        feature_client = feature_client.merge(features_set_column, on=column_client, how=\"outer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_client.to_csv(\"./data/feature_client.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_report = \"./reports/\"\n",
    "# if not os.path.isdir(folder_report):\n",
    "#     os.makedirs(folder_report)\n",
    "\n",
    "# path_report_features = os.path.join(folder_report, \"features.html\")\n",
    "# profile = ProfileReport(features_client)\n",
    "# profile.to_file(path_report_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lead Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lead_id(value):\n",
    "    if isinstance(value, str):\n",
    "        if value.startswith(\"Inventory Residential Lead - \"):\n",
    "            return int(value.split(\" - \")[-1])\n",
    "    return None\n",
    "leads_filtered[f\"{column_id_leads}_2\"] = leads_filtered[\"event_action\"].apply(get_lead_id)\n",
    "leads_filtered[column_id_target] = leads_filtered[column_id_leads].fillna(0) + leads_filtered[f\"{column_id_leads}_2\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating features for set: 1 column: sessions\n",
      "generating features for set: 1 column: hits\n",
      "generating features for set: 1 column: unique_events\n",
      "generating features for set: 1 column: session_duration\n",
      "generating features for set: 1 column: page_per_session\n",
      "generating features for set: 1 column: enquiries\n",
      "generating features for set: 1 column: pageviews\n",
      "generating features for set: 1 column: time_on_page\n",
      "generating features for set: 1 column: avg_time_on_page\n",
      "generating features for set: 1 column: property_search\n",
      "generating features for set: 1 column: lead_dropped\n",
      "generating features for set: 1 column: bounce_rate\n",
      "generating features for set: 1 column: total_events\n",
      "generating features for set: 2 column: timestamp\n",
      "generating features for set: 3 column: social_source_referral\n",
      "generating features for set: 4 column: contact_us_click\n",
      "generating features for set: 4 column: app_downloads\n",
      "generating features for set: 5 column: source\n",
      "generating features for set: 6 column: device_category\n",
      "generating features for set: 7 column: event_category\n"
     ]
    }
   ],
   "source": [
    "feature_leads = pd.DataFrame({column_id_target: []})\n",
    "leads = leads_filtered.groupby(column_id_target)\n",
    "\n",
    "for feature_set in leads_features_info:\n",
    "    feature_set_function = leads_features_info[feature_set][\"feature_function\"]\n",
    "    for column in leads_features_info[feature_set][\"columns\"]:\n",
    "        print(\"generating features for set:\", feature_set, \"column:\", column)\n",
    "        features_set_column = feature_set_function(leads, column, column_id_target)\n",
    "        feature_leads = feature_leads.merge(features_set_column, on=column_id_target, how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_leads.to_csv(\"./data/feature_leads.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging `target_data` with Lead Feature on Client Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lead IDs to consider: 3334\n",
      "shape of `target_filtered`: (3334, 2)\n",
      "shape of `feature_leads_filtered`: (3334, 139)\n",
      "shape of `features_combined`: (3334, 140)\n"
     ]
    }
   ],
   "source": [
    "# identifying required lead IDs from `leads_data` \n",
    "lead_ids_to_consider = leads_filtered[column_id_leads].unique()\n",
    "\n",
    "# filtering `target_data` based on required lead IDs\n",
    "target_filtered = target_filtered[target_filtered[column_id_target].isin(lead_ids_to_consider)]\n",
    "lead_ids_to_consider = target_filtered[column_id_target].unique()\n",
    "\n",
    "# filtering `leads_features` based on required lead IDs\n",
    "feature_leads_filtered = feature_leads[feature_leads[column_id_target].isin(lead_ids_to_consider)]\n",
    "\n",
    "# merging `target_filtered` with Lead Features\n",
    "features_combined = pd.merge(target_filtered, feature_leads_filtered, on=column_id_target, how=\"inner\")\n",
    "\n",
    "print(\"total lead IDs to consider:\", len(lead_ids_to_consider))\n",
    "print(\"shape of `target_filtered`:\", target_filtered.shape)\n",
    "print(\"shape of `feature_leads_filtered`:\", feature_leads_filtered.shape)\n",
    "print(\"shape of `features_combined`:\", features_combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client-lead ID combinations in `leads_data`: 3334\n"
     ]
    }
   ],
   "source": [
    "# extracting lead IDs and client IDs from `leads_data` for which we have `output` information in `target_data`\n",
    "client_leads = leads_filtered[[column_id_leads, column_client]].drop_duplicates()\n",
    "client_leads = client_leads[client_leads[column_id_leads].isin(target_filtered[column_id_target])]\n",
    "client_leads.columns = [column_id_target, column_client]\n",
    "\n",
    "print(\"client-lead ID combinations in `leads_data`:\", client_leads.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of `features` after combining with client-lead ID combinations: (3334, 141)\n",
      "shape of `features` after mergining with Client Features: (3334, 279)\n"
     ]
    }
   ],
   "source": [
    "# merging `features_combined` with extracted lead IDs and client IDs\n",
    "features = features_combined.merge(client_leads, on=column_id_target, how=\"outer\")\n",
    "print(\"shape of `features` after combining with client-lead ID combinations:\", features.shape)\n",
    "\n",
    "# merging `features` with Client Features\n",
    "features = features.merge(feature_client, on=column_client, how=\"inner\")\n",
    "print(\"shape of `features` after mergining with Client Features:\", features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To-Do\n",
    "Feature Cleaning\n",
    "<ol>\n",
    "    <li> Dropping features with constant value throughout </li>\n",
    "    <li> Dropping features with high correlation (above 90%) </li>\n",
    "    <li> Dropping data points with same feature information other than `lead_id`, `client_id` and `output` values </li>\n",
    "    <li> Dropping features having more than 70% of missing values </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of columns with zero variance: 68\n",
      "shape of `features` after dropping columns with zero variance: (3334, 211)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Archisha\\AppData\\Local\\Temp\\ipykernel_10984\\1935826333.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  feature_var_is_0 = (features.var() == 0).reset_index()\n"
     ]
    }
   ],
   "source": [
    "# dropping features with constant value throughout\n",
    "feature_var_is_0 = (features.var() == 0).reset_index()\n",
    "columns_with_var_0 = feature_var_is_0[feature_var_is_0[0]][\"index\"].tolist()\n",
    "features.drop(columns=columns_with_var_0, inplace=True)\n",
    "\n",
    "print(\"number of columns with zero variance:\", len(columns_with_var_0))\n",
    "print(\"shape of `features` after dropping columns with zero variance:\", features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features with high correlation: 137\n",
      "shape of `features` after dropping columns with correlation of over 90%: (3334, 74)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Archisha\\AppData\\Local\\Temp\\ipykernel_10984\\2054966160.py:4: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  corr_subset = feature_corr.where(np.triu(np.ones(feature_corr.shape), k=1).astype(np.bool))\n"
     ]
    }
   ],
   "source": [
    "# checking correlations amongst `features`\n",
    "feature_corr = features.corr().abs()\n",
    "# dropping features with high correlation (above 90%)\n",
    "corr_subset = feature_corr.where(np.triu(np.ones(feature_corr.shape), k=1).astype(np.bool))\n",
    "columns_with_high_corr = [column for column in corr_subset.columns if any(corr_subset[column] > 0.9)]\n",
    "features.drop(columns=columns_with_high_corr, inplace=True)\n",
    "\n",
    "print(\"number of features with high correlation:\", len(columns_with_high_corr))\n",
    "print(\"shape of `features` after dropping columns with correlation of over 90%:\", features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of features after dropping duplicate data points (2219, 74)\n"
     ]
    }
   ],
   "source": [
    "# dropping data points with same feature information and different `lead_id`, `client_id` and `output` values\n",
    "temp = features.drop(columns=[column_id_target, column_client, column_output])\n",
    "features = features.loc[temp.drop_duplicates().index]\n",
    "print(\"shape of features after dropping duplicate data points\",features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of final data after dropping duplicate data points (2219, 71)\n"
     ]
    }
   ],
   "source": [
    "# dropping features having more than 70% of missing values\n",
    "missing_values = (features.isna().mean()).reset_index()\n",
    "columns_to_drop = missing_values[missing_values[0]>0.7][\"index\"].to_list()\n",
    "features.drop(columns=columns_to_drop, inplace=True)\n",
    "print(\"shape of final data after dropping duplicate data points\",features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final data\n",
    "features.to_csv(\"./data/features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical columns in data: ['source-mode-client_id']\n"
     ]
    }
   ],
   "source": [
    "data = features.copy()\n",
    "\n",
    "# finding categorical columns in the data \n",
    "categorical_columns = [column for column in data.columns if not(is_numeric_dtype(data[column]))]\n",
    "print(\"categorical columns in data:\", categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding categorical columns\n",
    "label_encoder = {}\n",
    "\n",
    "for column in categorical_columns:\n",
    "    label_encoder[column] = preprocessing.LabelEncoder()\n",
    "    data[column].fillna(\"0\", inplace=True)\n",
    "    data[column] = label_encoder[column].fit_transform(data[column])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train data: (1775, 70) (1775,)\n",
      "shape of validation data: (222, 70) (222,)\n",
      "shape of test data: (222, 70) (222,)\n"
     ]
    }
   ],
   "source": [
    "# defining features and target with X and y\n",
    "X = data.drop(columns=[column_output]).copy()\n",
    "y = data[column_output]\n",
    "\n",
    "# creating train-val-test split\n",
    "# splitting data in training and remaining dataset\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X, y, train_size=0.8, random_state=226)\n",
    "\n",
    "# spliting remaining data into equal sections of validation and test split (15% each of overall data). \n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem, y_rem, test_size=0.5, random_state=226)\n",
    "\n",
    "print(\"shape of train data:\", X_train.shape, y_train.shape)\n",
    "print(\"shape of validation data:\", X_valid.shape, y_valid.shape)\n",
    "print(\"shape of test data:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping `client_id` and `lead_id` from train data, valid data and test data\n",
    "X_train.drop(columns=[column_id_target, column_client], inplace=True)\n",
    "\n",
    "X_valid_index = X_valid[[column_id_target, column_client]]\n",
    "X_test_index = X_test[[column_id_target, column_client]]\n",
    "\n",
    "X_valid.drop(columns=[column_id_target, column_client], inplace=True)\n",
    "X_test.drop(columns=[column_id_target, column_client], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train data: (2934, 68) (2934,)\n"
     ]
    }
   ],
   "source": [
    "# filling missing values with default value -1\n",
    "X_train.fillna(-1, inplace=True)\n",
    "X_valid.fillna(-1, inplace=True)\n",
    "X_test.fillna(-1, inplace=True)\n",
    "\n",
    "# using SMOTE to compensate for data scarcisty\n",
    "oversampling = SMOTE(random_state=226)\n",
    "X_train, y_train = oversampling.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"shape of train data:\", X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model scoring function - calculating f1 and roc-auc scores\n",
    "def scoring(model):\n",
    "    results = {}\n",
    "\n",
    "    y_predict = model.predict(X_train)\n",
    "    results[\"train-f1\"] = f1_score(y_predict, y_train)\n",
    "    results[\"train-roc_auc\"] = roc_auc_score(y_predict, y_train)\n",
    "\n",
    "    y_predict = model.predict(X_valid)\n",
    "    results[\"valid-f1\"] = f1_score(y_predict, y_valid)\n",
    "    results[\"valid-roc_auc\"] = roc_auc_score(y_predict, y_valid)\n",
    "\n",
    "    y_predict = model.predict(X_test)\n",
    "    results[\"test-f1\"] = f1_score(y_predict, y_test)\n",
    "    results[\"test-roc_auc\"] = roc_auc_score(y_predict, y_test)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a list to store all model results\n",
    "model_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 and ROC-AUC score for train, validation and test data: {'train-f1': 0.6305170239596469, 'train-roc_auc': 0.6032632686613105, 'valid-f1': 0.36486486486486486, 'valid-roc_auc': 0.570755865876431, 'test-f1': 0.3096774193548387, 'test-roc_auc': 0.5358974358974359}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Archisha\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# training logistic regression with no hyperparameter tuning as the baseline model\n",
    "baseline_dt = LogisticRegression(random_state=0)\n",
    "baseline_dt.fit(X_train, y_train)\n",
    "\n",
    "baseline_dt_score = scoring(baseline_dt)\n",
    "print(\"F1 and ROC-AUC score for train, validation and test data:\", baseline_dt_score)\n",
    "\n",
    "result_df = pd.DataFrame(baseline_dt_score.items()).set_index(0).T.reset_index()\n",
    "result_df[\"model\"] = \"Logistic Regression\"\n",
    "result_df[\"params\"] = \"baseline\"\n",
    "model_results.append(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating F1 and ROC-AUC score for train, validation and test data along with different hyperparameter values\n"
     ]
    }
   ],
   "source": [
    "# training decision tree with hyperparameter tuning and storing the results for model benchmarking\n",
    "print(\"calculating F1 and ROC-AUC score for train, validation and test data along with different hyperparameter values\")\n",
    "\n",
    "for depth in range(4, 15):\n",
    "    tree_dt = DecisionTreeClassifier(max_depth=depth, random_state=0)\n",
    "    tree_dt.fit(X_train, y_train)\n",
    "    \n",
    "    result = scoring(tree_dt)\n",
    "    params = f\"depth:{depth}\"\n",
    "\n",
    "    result_df = pd.DataFrame(result.items()).set_index(0).T.reset_index()\n",
    "    result_df[\"model\"] = \"Decision Tree\"\n",
    "    result_df[\"params\"] = params\n",
    "    model_results.append(result_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating F1 and ROC-AUC score for train, validation and test data along with different hyperparameter values\n"
     ]
    }
   ],
   "source": [
    "# training KNN with hyperparameter tuning and storing the results for model benchmarking\n",
    "print(\"calculating F1 and ROC-AUC score for train, validation and test data along with different hyperparameter values\")\n",
    "\n",
    "neighbors = [10, 20, 50, 80, 100, 150]\n",
    "leaf_sizes = [10, 20, 50]\n",
    "for neighbor in neighbors:\n",
    "    for leaf_size in leaf_sizes:\n",
    "        nei_knn = KNeighborsClassifier(n_neighbors = neighbor, weights='uniform',leaf_size=leaf_size)\n",
    "        nei_knn.fit(X_train, y_train)\n",
    "\n",
    "        result = scoring(nei_knn)\n",
    "        params = f\"neighbor:{neighbor}, leaf_size:{leaf_size}\"\n",
    "\n",
    "        result_df = pd.DataFrame(result.items()).set_index(0).T.reset_index()\n",
    "        result_df[\"model\"] = \"KNN\"\n",
    "        result_df[\"params\"] = params\n",
    "        model_results.append(result_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating F1 and ROC-AUC score for train, validation and test data along with different hyperparameter values\n"
     ]
    }
   ],
   "source": [
    "print(\"calculating F1 and ROC-AUC score for train, validation and test data along with different hyperparameter values\")\n",
    "\n",
    "learning_rates = [0.01, 0.05, 0.1]\n",
    "n_estimators = [10, 25, 50, 100, 150]\n",
    "min_samples_splits = [10, 25, 50]\n",
    "max_depths = [4, 7, 10, 15]\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for n_estimator in n_estimators:\n",
    "        for min_samples_split in min_samples_splits:\n",
    "            for max_depth in max_depths:\n",
    "                model_gb = GradientBoostingClassifier(\n",
    "                    random_state = 0, \n",
    "                    n_estimators = n_estimator, \n",
    "                    learning_rate = learning_rate,\n",
    "                    min_samples_split = min_samples_split,\n",
    "                    max_depth = max_depth\n",
    "                )\n",
    "\n",
    "                model_gb.fit(X_train, y_train)\n",
    "\n",
    "                result = scoring(model_gb)\n",
    "                params = f\"learning_rate:{learning_rate}, n-estimators:{n_estimator}, min_samples_split:{min_samples_split}, max_depth:{max_depth}\"\n",
    "\n",
    "                result_df = pd.DataFrame(result.items()).set_index(0).T.reset_index()\n",
    "                result_df[\"model\"] = \"GBM\"\n",
    "                result_df[\"params\"] = params\n",
    "                model_results.append(result_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating F1 and ROC-AUC score for train, validation and test data along with different hyperparameter values\n"
     ]
    }
   ],
   "source": [
    "print(\"calculating F1 and ROC-AUC score for train, validation and test data along with different hyperparameter values\")\n",
    "\n",
    "ne_list = [10, 20, 50, 80, 100, 150, 200]\n",
    "\n",
    "for est in ne_list:\n",
    "    for depth in range(4, 10):\n",
    "\n",
    "        tree_rf = RandomForestClassifier(n_estimators=est, max_depth=depth, random_state=0)\n",
    "        tree_rf.fit(X_train, y_train)\n",
    "        \n",
    "        result = scoring(tree_rf)\n",
    "        params = f\"n_estimators:{est}, max_depth:{depth}\"\n",
    "\n",
    "        result_df = pd.DataFrame(result.items()).set_index(0).T.reset_index()\n",
    "        result_df[\"model\"] = \"Random Forest\"\n",
    "        result_df[\"params\"] = params\n",
    "        model_results.append(result_df)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concating all the model results\n",
    "model_eval_df = pd.concat(model_results, ignore_index=True)\n",
    "df_columns = model_eval_df.columns[1:].tolist()\n",
    "model_eval_df = model_eval_df[df_columns[-2:] + df_columns[:-2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>params</th>\n",
       "      <th>train-f1</th>\n",
       "      <th>train-roc_auc</th>\n",
       "      <th>valid-f1</th>\n",
       "      <th>valid-roc_auc</th>\n",
       "      <th>test-f1</th>\n",
       "      <th>test-roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>GBM</td>\n",
       "      <td>learning_rate:0.01, n-estimators:10, min_sampl...</td>\n",
       "      <td>0.732240</td>\n",
       "      <td>0.732792</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.621144</td>\n",
       "      <td>0.330579</td>\n",
       "      <td>0.555734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>GBM</td>\n",
       "      <td>learning_rate:0.01, n-estimators:10, min_sampl...</td>\n",
       "      <td>0.732240</td>\n",
       "      <td>0.732792</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.621144</td>\n",
       "      <td>0.330579</td>\n",
       "      <td>0.555734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>GBM</td>\n",
       "      <td>learning_rate:0.01, n-estimators:10, min_sampl...</td>\n",
       "      <td>0.732240</td>\n",
       "      <td>0.732792</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.621144</td>\n",
       "      <td>0.330579</td>\n",
       "      <td>0.555734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>n_estimators:50, max_depth:4</td>\n",
       "      <td>0.770349</td>\n",
       "      <td>0.789043</td>\n",
       "      <td>0.426667</td>\n",
       "      <td>0.660394</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.611413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>n_estimators:100, max_depth:4</td>\n",
       "      <td>0.774542</td>\n",
       "      <td>0.789447</td>\n",
       "      <td>0.430380</td>\n",
       "      <td>0.652060</td>\n",
       "      <td>0.417582</td>\n",
       "      <td>0.623032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>n_estimators:100, max_depth:5</td>\n",
       "      <td>0.809456</td>\n",
       "      <td>0.821692</td>\n",
       "      <td>0.422535</td>\n",
       "      <td>0.671217</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.620851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>n_estimators:150, max_depth:4</td>\n",
       "      <td>0.773754</td>\n",
       "      <td>0.787746</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.657839</td>\n",
       "      <td>0.422222</td>\n",
       "      <td>0.626810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>n_estimators:200, max_depth:4</td>\n",
       "      <td>0.769010</td>\n",
       "      <td>0.783311</td>\n",
       "      <td>0.441558</td>\n",
       "      <td>0.663902</td>\n",
       "      <td>0.413043</td>\n",
       "      <td>0.619378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0            model                                             params  \\\n",
       "30             GBM  learning_rate:0.01, n-estimators:10, min_sampl...   \n",
       "34             GBM  learning_rate:0.01, n-estimators:10, min_sampl...   \n",
       "38             GBM  learning_rate:0.01, n-estimators:10, min_sampl...   \n",
       "222  Random Forest                       n_estimators:50, max_depth:4   \n",
       "234  Random Forest                      n_estimators:100, max_depth:4   \n",
       "235  Random Forest                      n_estimators:100, max_depth:5   \n",
       "240  Random Forest                      n_estimators:150, max_depth:4   \n",
       "246  Random Forest                      n_estimators:200, max_depth:4   \n",
       "\n",
       "0    train-f1  train-roc_auc  valid-f1  valid-roc_auc   test-f1  test-roc_auc  \n",
       "30   0.732240       0.732792  0.431373       0.621144  0.330579      0.555734  \n",
       "34   0.732240       0.732792  0.431373       0.621144  0.330579      0.555734  \n",
       "38   0.732240       0.732792  0.431373       0.621144  0.330579      0.555734  \n",
       "222  0.770349       0.789043  0.426667       0.660394  0.380952      0.611413  \n",
       "234  0.774542       0.789447  0.430380       0.652060  0.417582      0.623032  \n",
       "235  0.809456       0.821692  0.422535       0.671217  0.400000      0.620851  \n",
       "240  0.773754       0.787746  0.435897       0.657839  0.422222      0.626810  \n",
       "246  0.769010       0.783311  0.441558       0.663902  0.413043      0.619378  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identifying the best model \n",
    "model_eval_df[(model_eval_df[\"valid-roc_auc\"]>0.62) & (model_eval_df[\"valid-f1\"]>0.42)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "To get the best two models we are looking at `valid-roc_auc` greater than 62% and from above result we can conclude \n",
    "* Random Forest model with 100 n-estimators and 5 as the maximum depth\n",
    "* Gradient Boost model with learning_rate=0.01, n-estimators=10, min_samples_split=25 and max_depth=4\n",
    "\n",
    "are the best and avoid cases of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success percentage without machine learning: 17.56756756756757\n"
     ]
    }
   ],
   "source": [
    "# finding leads conversion rate without using machine learning\n",
    "default_success_percentage = 100 * y_valid.mean()\n",
    "print(\"success percentage without machine learning:\", default_success_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_probability(model, x):\n",
    "    return model.predict_proba(x)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingClassifier(learning_rate=0.01, max_depth=4,\n",
       "                           min_samples_split=25, n_estimators=10,\n",
       "                           random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(learning_rate=0.01, max_depth=4,\n",
       "                           min_samples_split=25, n_estimators=10,\n",
       "                           random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingClassifier(learning_rate=0.01, max_depth=4,\n",
       "                           min_samples_split=25, n_estimators=10,\n",
       "                           random_state=0)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding leads conversion rate using machine learning\n",
    "final_models = {}\n",
    "\n",
    "final_models[\"Random Forest\"] = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)\n",
    "final_models[\"Random Forest\"].fit(X_train, y_train)\n",
    "\n",
    "final_models[\"Gradient Boosting Model\"] = GradientBoostingClassifier(learning_rate=0.01, n_estimators=10, min_samples_split=25, max_depth=4, random_state=0)\n",
    "final_models[\"Gradient Boosting Model\"].fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success percentage for top 50 picks using Random Forest: 36.0\n",
      "we can increase leads conversion rate for top 50 picks by: 18.43243243243243\n",
      "===================================================\n",
      "success percentage for top 100 picks using Random Forest: 27.0\n",
      "we can increase leads conversion rate for top 100 picks by: 9.432432432432432\n",
      "===================================================\n",
      "success percentage for top 50 picks using Gradient Boosting Model: 40.0\n",
      "we can increase leads conversion rate for top 50 picks by: 22.43243243243243\n",
      "===================================================\n",
      "success percentage for top 100 picks using Gradient Boosting Model: 24.0\n",
      "we can increase leads conversion rate for top 100 picks by: 6.432432432432432\n",
      "===================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lead_id</th>\n",
       "      <th>lead_success_score</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2386</th>\n",
       "      <td>299054</td>\n",
       "      <td>0.539002</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2250</th>\n",
       "      <td>298769</td>\n",
       "      <td>0.539002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>287846</td>\n",
       "      <td>0.513764</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2557</th>\n",
       "      <td>290481</td>\n",
       "      <td>0.513764</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>296518</td>\n",
       "      <td>0.513764</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lead_id  lead_success_score  ground_truth\n",
       "2386   299054            0.539002             1\n",
       "2250   298769            0.539002             0\n",
       "7      287846            0.513764             0\n",
       "2557   290481            0.513764             1\n",
       "922    296518            0.513764             1"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for model_name, model in final_models.items():\n",
    "    # scoring leads and identifying top picks\n",
    "    scoring_df = pd.DataFrame({\n",
    "        column_id_target: X_valid_index[column_id_target],\n",
    "        \"lead_success_score\": get_prediction_probability(model, X_valid),\n",
    "        \"ground_truth\": y_valid\n",
    "    }).sort_values(by=\"lead_success_score\", ascending=False)\n",
    "\n",
    "    top_picks = [50, 100]\n",
    "    for top_pick in top_picks:\n",
    "        success_percentage = 100 * scoring_df.iloc[:top_pick][\"ground_truth\"].mean()\n",
    "        print(f\"success percentage for top {top_pick} picks using {model_name}:\", success_percentage)\n",
    "        print(f\"we can increase leads conversion rate for top {top_pick} picks by:\", success_percentage - default_success_percentage)\n",
    "        print(\"===================================================\")\n",
    "\n",
    "scoring_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "From the above results on validation data we conclude that Random Forest Model performs better than Gradient Boosting Model <br>\n",
    "\n",
    "<b> Model selected - Random Forest Model </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring on test data (using selected model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success percentage on test set without machine learning: 17.117117117117118\n",
      "success percentage for top 50 test picks using Random Forest: 36.0\n",
      "we can increase leads conversion rate for top 50 test picks by: 18.882882882882882\n"
     ]
    }
   ],
   "source": [
    "test_success_percentage = 100 * y_test.mean()\n",
    "\n",
    "model_name = \"Random Forest\"\n",
    "final_model = final_models[model_name]\n",
    "# scoring leads using probablities and identifying top picks\n",
    "scoring_df = pd.DataFrame({\n",
    "    column_id_target: X_test_index[column_id_target],\n",
    "    \"lead_success_score\": get_prediction_probability(final_model, X_test),\n",
    "    \"ground_truth\": y_test\n",
    "}).sort_values(by=\"lead_success_score\", ascending=False)\n",
    "\n",
    "top_pick = 50\n",
    "success_percentage = 100 * scoring_df.iloc[:top_pick][\"ground_truth\"].mean()\n",
    "print(\"success percentage on test set without machine learning:\", test_success_percentage)\n",
    "print(f\"success percentage for top {top_pick} test picks using {model_name}:\", success_percentage)\n",
    "print(f\"we can increase leads conversion rate for top {top_pick} test picks by:\", success_percentage - test_success_percentage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sessions-sum_per_lead-client_id</td>\n",
       "      <td>0.078356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>bounce_rate-max-client_id</td>\n",
       "      <td>0.074881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>bounce_rate-sum-client_id</td>\n",
       "      <td>0.062534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lead_dropped-mean-lead_id</td>\n",
       "      <td>0.056392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>bounce_rate-mean-client_id</td>\n",
       "      <td>0.048686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>session_duration-mean-client_id</td>\n",
       "      <td>0.042267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>session_duration-max-client_id</td>\n",
       "      <td>0.040844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>source-mode-client_id</td>\n",
       "      <td>0.034769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>timestamp-gap-client_id</td>\n",
       "      <td>0.034032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>property_search-mean-client_id</td>\n",
       "      <td>0.029067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             column  importance\n",
       "17  sessions-sum_per_lead-client_id    0.078356\n",
       "43        bounce_rate-max-client_id    0.074881\n",
       "42        bounce_rate-sum-client_id    0.062534\n",
       "9         lead_dropped-mean-lead_id    0.056392\n",
       "44       bounce_rate-mean-client_id    0.048686\n",
       "22  session_duration-mean-client_id    0.042267\n",
       "21   session_duration-max-client_id    0.040844\n",
       "50            source-mode-client_id    0.034769\n",
       "46          timestamp-gap-client_id    0.034032\n",
       "38   property_search-mean-client_id    0.029067"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importace = pd.DataFrame({\"column\": X_train.columns, \"importance\": final_model.feature_importances_})\n",
    "feature_importace.sort_values(by=\"importance\", ascending=False, inplace=True)\n",
    "feature_importace.iloc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "* We have successfully built a scoring mechanism that will help the sales executives prioritize the leads using Random Forest\n",
    "* Originally the data had a 17% success rate of converting leads to tenants\n",
    "* Using this model we can achieve a 19% increase, i.e a total of 36% success rate in lead to tenant conversion while selecting top 50 leads\n",
    "* Insights for the client: Recording more information on the following will help online behavior analysis and improve scoring model\n",
    "    * Duration of sessions directly affects the conversion rate \n",
    "    * Number of sessions directly affects the conversion rate\n",
    "    * Bounce rate inversely affects the conversion rate\n",
    "    * Some categories of `Source` directly affects the conversion rate\n",
    "    * Lead ID assignment methodology - understand and refine the logic for assigning lead IDs eg: a lead ID is assigned to only some rows of the same client even though the others contain same information"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a7cd216025838b8c10ffbc8aea7d02e0e5159a942723fa6bb1187f46cac30190"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
